{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddhanthnagrath1/MINI-PROJECT-WEB-CRAWLING/blob/main/web_crawling_mt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-26T10:04:35.948026Z",
          "iopub.status.busy": "2022-12-26T10:04:35.947633Z",
          "iopub.status.idle": "2022-12-26T10:05:06.892132Z",
          "shell.execute_reply": "2022-12-26T10:05:06.890969Z",
          "shell.execute_reply.started": "2022-12-26T10:04:35.947994Z"
        },
        "trusted": true,
        "id": "FNq6jU_HjaG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3f6361-01a6-4b34-a504-176cc9e44c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.31.0)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Collecting bs4 (from requests-html)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.2.1-py3-none-any.whl (21 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (2024.6.2)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.0.0)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-11.1.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.66.4)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->requests-html) (4.12.3)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests-html) (4.9.4)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->requests-html) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->requests-html) (3.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.19.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->requests-html) (2.5)\n",
            "Installing collected packages: parse, fake-useragent, appdirs, websockets, w3lib, urllib3, pyee, cssselect, pyquery, pyppeteer, bs4, requests-html\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "Successfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.2.0 fake-useragent-1.5.1 parse-1.20.2 pyee-11.1.0 pyppeteer-2.0.0 pyquery-2.0.0 requests-html-0.10.0 urllib3-1.26.19 w3lib-2.2.1 websockets-10.4\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement request (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for request\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests-html\n",
        "!pip install re\n",
        "!pip install request\n",
        "!pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-26T10:05:06.904446Z",
          "iopub.status.busy": "2022-12-26T10:05:06.903610Z",
          "iopub.status.idle": "2022-12-26T10:05:06.918106Z",
          "shell.execute_reply": "2022-12-26T10:05:06.916982Z",
          "shell.execute_reply.started": "2022-12-26T10:05:06.904409Z"
        },
        "trusted": true,
        "id": "BPUO7soCjaHB"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import deque\n",
        "from urllib.parse import urlsplit\n",
        "import pandas as pd\n",
        "from requests_html import HTMLSession\n",
        "import threading\n",
        "import time\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-26T10:05:06.921811Z",
          "iopub.status.busy": "2022-12-26T10:05:06.921045Z",
          "iopub.status.idle": "2022-12-26T10:05:06.933536Z",
          "shell.execute_reply": "2022-12-26T10:05:06.931924Z",
          "shell.execute_reply.started": "2022-12-26T10:05:06.921767Z"
        },
        "trusted": true,
        "id": "O8XrpB4WjaHF"
      },
      "outputs": [],
      "source": [
        "#user_url = \"https://www.thapar.edu/sitemap.xml\"\n",
        "#url = [\"https://www.thapar.edu\"]\n",
        "EMAIL_REGEX = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-26T10:05:06.936434Z",
          "iopub.status.busy": "2022-12-26T10:05:06.936048Z",
          "iopub.status.idle": "2022-12-26T10:05:09.351623Z",
          "shell.execute_reply": "2022-12-26T10:05:09.350425Z",
          "shell.execute_reply.started": "2022-12-26T10:05:06.936398Z"
        },
        "trusted": true,
        "id": "xw51GoK6jaHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40f0f7c-bf54-4f49-f436-903fb54e5aea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-1f737a625830>:4: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  soup = BeautifulSoup(xml)\n"
          ]
        }
      ],
      "source": [
        "def get_urls_of_xml(xml_url):\n",
        "    r = requests.get(xml_url)\n",
        "    xml = r.text\n",
        "    soup = BeautifulSoup(xml)\n",
        "\n",
        "    links_arr = []\n",
        "    for link in soup.findAll('loc'):\n",
        "        linkstr = link.getText('', True)\n",
        "        links_arr.append(linkstr)\n",
        "\n",
        "    return links_arr\n",
        "\n",
        "\n",
        "\n",
        "links_data_arr = get_urls_of_xml(\"https://thapar.edu/sitemap.xml\")\n",
        "#print(links_data_arr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-26T10:05:09.357183Z",
          "iopub.status.busy": "2022-12-26T10:05:09.356725Z",
          "iopub.status.idle": "2022-12-26T10:05:09.364213Z",
          "shell.execute_reply": "2022-12-26T10:05:09.362512Z",
          "shell.execute_reply.started": "2022-12-26T10:05:09.357144Z"
        },
        "trusted": true,
        "id": "2p6hNPpAjaHM"
      },
      "outputs": [],
      "source": [
        "session = HTMLSession()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-26T10:05:17.089462Z",
          "iopub.status.busy": "2022-12-26T10:05:17.089017Z",
          "iopub.status.idle": "2022-12-26T10:17:47.278602Z",
          "shell.execute_reply": "2022-12-26T10:17:47.277052Z",
          "shell.execute_reply.started": "2022-12-26T10:05:17.089423Z"
        },
        "trusted": true,
        "id": "i1gl4dsJjaHO"
      },
      "outputs": [],
      "source": [
        "email=set()\n",
        "for i in tqdm(links_data_arr):\n",
        "        #requests.get(f'{i}', verify=False)\n",
        "        r = session.get(i)\n",
        "        for re_match in re.finditer(EMAIL_REGEX, r.html.raw_html.decode()):\n",
        "            email.add(((re_match.group())).replace(\"-\",\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-26T10:18:56.632354Z",
          "iopub.status.busy": "2022-12-26T10:18:56.631963Z",
          "iopub.status.idle": "2022-12-26T10:18:56.641790Z",
          "shell.execute_reply": "2022-12-26T10:18:56.640417Z",
          "shell.execute_reply.started": "2022-12-26T10:18:56.632323Z"
        },
        "trusted": true,
        "id": "QY1HjTyfjaHQ"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(email)\n",
        "df.to_csv('Emails.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}